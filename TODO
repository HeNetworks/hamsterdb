I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.8 ..................................................
x hamsterdb pro: compression for journal
x hamsterdb pro: compression for records
x hamsterdb pro: aes encryption
o hamsterdb pro: compression for keys
o hamsterdb pro: 30 day evaluation
o hamsterdb: linear searches
o hamsterdb pro: SIMD instructions
o hamsterdb: more quickcheck tests



o web-page requires updates
    x deployed html differs from git-repository
    x download/sources: add erlang, remove 1.x
    x www1-repository and hamsterdb-www should be identical
        x updates for 2.1.6 are missing
        x samples fehlen
        x doku fehlt
        x download-dateien fehlen
    x www1
        x www1 and www2 are already combined in a single remote target
        x where to host static files? - hetzner server
        x clean up 'dl' directory
    x webpage copyright is still 2013
    o hamsterdb.com has read-only access to git repository, can do "git pull"
        for deployment
        "deploy" -> openshift, dann auch hamsterdb.com
================
        "staging" -> staging.hamsterdb.com
        auf dem server noch fertig einrichten
================

    o same for staging.hamsterdb.com, but uses the staging-branch
    o merge all repositories (host on github, keep remote branch)
        hamsterdb-www is the main repository, master pushes to staging
        'stable' pushes to hamsterdb.com, www1, www2

x PRO: enable zlib/snappy/others compression for the records
    x add documentation to header file
    x db: add record compression parameter
        x when creating a database
        x when opening a database
    x create compressor in database (w/ autoptr)
    x refactor BlobManager: should have common functionality, subclasses
            implement do_overwrite, do_allocate etc
        x move parameter checks and duplicate code to parent class
        x do not allow partial updates if compression is enabled
            x unittests
    x implement the actual compression/decompression
        x for in-memory AND disk
        x compression: store flag in BlobHeader
        x only compress if compressed size < original size
        x directly uncompress into output buffer,
            not in internal buffer of the compressor
            (or into user allocated buffer)
        x add unittests (disk and inmemory)
    x add parameters to ham_bench
    x add perftests, monster tests

x PRO: should journal and record compression settings really
    be non-persistent?
    -> should be persistent, makes things easier
    x journal compression: store in env-header
        x env-header has 2 reserved bytes, use them for compression
    x journal compression: disallow in ham_env_open
        x unittest
    x record compression: store in db-header
        x BtreeHeader has 2 bytes reserved for padding, use them for compression
    x record compression: disallow in ham_env_open_db
        x unittest
    x support ham_env_get_parameters
        x unittest
    x support ham_db_get_parameters
        x unittest
    x fix ham_bench
    x add ham_*_get_parameters to header documentation
    x fix memory leaks in unittests

x PRO: is able to open APL files, but not vice versa
    x use msb in file version for a marker
    x when loading a APL file in PRO: set the flag, write back
    x APL will automatically reject the file
    x ham_info: show whether db is pro
    x ham_info should print compression information
    x make ham_info a 100% user of the public API; do not access
        internal classes!

x do we really need the compression LEVEL parameters? if not then we can
    store the key compression flags very conveniently. otherwise we
    need extra storage! - no, remove it

o PRO: heavyweight compression for keys
    -> support compression with lzf and the other libraries
    -> keys are stored compressed IF the compressed keys are smaller than the
        uncompressed keys
    -> key->size in the node is the compressed size, and it might further
            be extended! the uncompressed size is part of the payload (16bit
            at the front)
    o compress before inserting
        x need 2 key flags; for 1byte length indicator and for 2bytes
        o store the uncompressed size up-front (1 byte if uncompressed size
            < 256, otherwise 2 bytes)
        o try to move the logic to a Compressor template class
    o uncompress in get_key(), get_key_direct(), compare() etc
    o test in combination with extended keys (very low threshold)
    o metrics: keep track of compression ratio, number of compressed and
        uncompressed keys
        o same for records and journals?
    o cache the uncompressed keys?

o use linear search for PAX (see topic/linear), but improve the
    performance!

o PRO: use SIMD instructions for the linear search
    http://openproceedings.org/EDBT/2014/paper_107.pdf

o default layout: the lookup operations are relatively slow compared to PAX;
    can we somehow improve this?

o insert: if HAM_HINT_APPEND is set then page splits should only create a
    new page, and not shifting any keys to the new sibling. only the new
    key is inserted in the new sibling.

o PRO: 30 day evaluation library
    o ham_time_t ham_is_evaluation(); -> returns end time, or 0
    o insert 30 day trial check macros at various positions
        o dynamically generate those macros
    o needs different licensing output in the tools
    o print to stdout in ham_env_create, ham_env_open
    o evaluate source obfuscators
        Stunnix - http://www.stunnix.com/prod/cxxo/pricing.shtml
        Mangle-It - http://www.brothersoft.com/mangle-it-c%2B%2B-obfuscator-91793.html
        Semantic Design - http://www.semdesigns.com/Purchase/
        o also test the performance!
    o create one source/win32 package starting at each week, running
            4 weeks
        o requires a build tool for unix and win32
        o on windows only build the DLLs, then run a quick test
                (subset of the unittests)
                and also check ham_is_evaluation(), ham_is_pro()
        o use source obfuscator
        o ... but also rename the files (f01.cc, f02.cc, ...)
    o create automagically in release-build.pl

o PRO: work over the license agreement; PRO will be similar to an NDA

o PRO: extend release-tool
    o new switch --product=apl|pro (default: apl)
    o for pro: use different version tag
    o for pro: use different packaging directory
    o for pro: use different file list
    o for pro: add test for --disable-compression
    o for pro: add test for --disable-encryption
    o for pro: generate 30 day trials for the following 3 months,
        starting at every calendar week

o PRO: additional stuff for the first release
    o needs a ChangeLog

    o create new documentation page (wiki) for pro (tutorial + ham_bench)
        o journal compression
        o record compression
        o zlib compression
        o aes encryption
        o file compatibility
        o ...

    o webpage
        o overview, catch lines
        o customers, deployments
        o feature matrix APL vs. closed source
        o daily backups of the database
        o can download commercial files using the admin-API to verify the
            login

    o what are the minimum features required for the first release?
        x heavyweight compression for the journal
        x AES encryption for the file
        x heavyweight compression for records
        o file format compatibility
        o heavyweight compression for keys
        o SIMD for searches (PAX)
        o (evaluation licenses)

------------- first release of hamsterdb-pro 2.1.7 --------------------------

o QuickCheck: create a new property for testing duplicates; the key is always
        the same. The number of duplicate keys is tracked and periodically
        checked with the API. A cursor can be used to remove a specific
        duplicate, or to fetch a specific duplicate.

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o delta updates managed in the BtreeNode
    the catch is that the actual insert operation is separated from
    the user's insert operation. The page will be logged and flushed,
    but the update is not performed.
    -> make sure the page is not "dirty" if the delta update is appended,
    -> ... only when it's merged
    -> then how will it be added to the changeset?
    -> is a page dirty if it has DUs, but the payload was not modified? - no!

    x perform a benchmark/profiling: random inserts vs. ascending inserts;
        the difference should be caused by memcopy/memmove (is it?)
        x PAX
            -> absolutely worth the effort, about 60% are spent in memmove
        x Default
            -> also worth the effort, about 15% are spent in memmove

    o if the deltas are merged then the page must be added to the
        changeset; invest more time in thinking about recovery and
        make this water proof!!
        -> also think about hot backups and how they can work with DUs
        -> no clue yet on how to make this water proof.
            the DUs are logged in the journal. As long as the DUs are not
            flushed, the journal also must remain. But it's non-trivial
            to figure out when the journal can be cleaned up, because the
            DUs are not flushed in order of their lsn!
        -> we could log the DUs separately from the journal, but this would
            cause additional I/O and cleaning up the journal would not become
            easier

        => whenever DUs are flushed: write the lsn's of all merged DUs
            to the log (as part of the changeset). When recovering then 
            skip those DUs and do not re-apply them. (alternatively:
            re-applying must be idempotent. but that sounds more complex.)
            => the lsn of the last operation is currently stored in the
                page; but this lsn is no longer relevant for skipping
                operations during recovery.
        => when to clean up the journal? we could keep track of all flushed
            LSNs, but that's too much effort. We could also count the
            DUs that were flushed; if all DUs with a lsn in the range of a
            specific file were flushed then we know we're done.

    o need a flag to disable DeltaUpdates
    o rename TransactionOperation to DeltaUpdate, decouple code from txn
    o totally transparent to the caller, handled in the proxy
    o merge them when reading and flushing
    o requires_split() takes delta updates into account
    o ham_env_flush also flushes the DUs 
    o only add deltas to leaf nodes; internal nodes have too many read
        operations and would anyway require immediate flushes
    o DeltaUpdate objects from a txn-flush should immediately go down to
        the node
    o make the merge algorithm more efficient
        o sort deltas by key
        o first execute all 'erase' either against other deltas or against
            the node
        o then merge the remaining inserts
        o this needs lots of tests

. pro: admin webpage
    o database is in openshift
    o admin webpage is secured by htaccess (has a non-standard path)
    o has a dashboard
        o number of customers
        o number of customers which will expire in less than 1 month
        o number of trial customers
        o number of trial customers which will expire in less than 1 week
    o has a table with the releases
    o has a table with the customers and their license status
            (trial from-to, customer from-to)
    o has a table with the links/guids for the customers
    o can list, add, edit, remove releases
    o can list, add, edit, remove customers
    o can create a new trial customer
    o can create a new (non-trial) customer
    o can upgrade a customer
    o can list trial customers which expire soon
    o can list real customers which expire soon
    o has an api to verify the GUIDs
    o daily backup of the database to the other server

    o need a second installation for testing!

    . expired 30day licenses will be removed automatically from the server
        o and from the backup server
        o and from the database

. collect file format updates
    o PageManager state: also store m_last_blob_page_id persistently
        (try not to break backwards compatibility, though)
    o EnvHeader is completely full; add at least 8 bytes for flags + reserved
    o reserve CRC32 for each page

o PRO: CRC32-checksums (as soon as the file format is updated)

o PRO: bulk updates
    - require delta updates
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort
    - add those delta updates to the txn-trees or to the btree node,
        transfer ownership of the memory

o PRO: hot backups (vacuumizes to a different file)
    - copies the database file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    - then applies all committed transactions to the other file
    --> think this through; how to deal with delta updates?
        what if only a few databases should be backed up?
        what if i want to back up in a neutral format (i.e. csv)?

o PRO: prefix compression for variable-length keys
    - if key is appended at the end: just write the delta
    - otherwise things get tricky, because the keys to the "right" of the
        inserted key have to be re-compressed; and in this case it is
        possible that the space will overflow
    - when re-compressing: key size might grow or shrink, which means that
        the keys "to the right" should be rearranged as well
    ===> it really makes sense to have delta updates and bulk inserts first!!
    - try to store a full key every Nth bytes, to avoid extensive re-compressing
    - full keys can be further compressed with lzf or lzo
    - key search: jumps from full key to full key; in between, there's a
        linear search for the key

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?

- cache-oblivious page distribution? -> PRO
    http://supertech.csail.mit.edu/cacheObliviousBTree.html
    see below
- bloom filter -> PRO
- concurrency -> PRO

o the bucket for concurrency TODOs
    o reduce the linked lists - they're hard to be updated with atomic
        operations
        o page
        o transaction and dependent objects
        o ...

    . come up with a list of all functions, define which locking operation
        is required; then review the code and make sure this will work
    . come up with a list of functions for which concurrency makes most sense
        - parallel lookups (using the same memory arena)
        - flushing transactions asynchronously
        - purging caches asynchronously
        - async. merging of delta updates
        - have concurrent lookups/find

    o separate SMOs from the actual operation (#2)
        -> check the literature
        http://pdf.aminer.org/000/409/763/b_trees_with_relaxed_balance.pdf
        o move SMO operations to "the janitor" (btree_janitor.h)

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

o btree_impl_default::set_record: if the duplicate is LAST of the last key
    in the node then simply append the record and increase next_offset

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for Transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)
